{"learning_rate": 1e-05, "per_gpu_train_batch_size": 4, "gradient_accumulation_steps": 8, "sent_lambda": 5, "frozen_layer_number": 18, "gnn_drop": 0.3, "bi_attn_drop": 0.3, "trans_drop": 0.25, "lstm_drop": 0.3, "num_train_epochs": 12, "model_type": "roberta", "encoder_name_or_path": "roberta-large", "seed": 42, "exp_name": "train.roberta.bs4lr1e-05.seed42"}